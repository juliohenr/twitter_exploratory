{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "from unicodedata import normalize as norm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for exploratory analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_count_words(text_column=None,\n",
    "                         label_column=None,\n",
    "                         name_class=None,\n",
    "                         dataframe=None,\n",
    "                         metric='SUM',\n",
    "                         top=50,return_df=True):\n",
    "    \n",
    "    corpus = dataframe[text_column].values\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    data_vect = vectorizer.fit_transform(corpus)\n",
    "    data_vect = data_vect.toarray()\n",
    "    \n",
    "    df_count_words =  pd.DataFrame({\n",
    "    \"WORDS\":vectorizer.get_feature_names(),\n",
    "    \"MEAN\":data_vect.mean(axis=0),\n",
    "    \"SUM\":data_vect.sum(axis=0),\n",
    "    \"STD\":data_vect.std(axis=0),\n",
    "    }) \n",
    "    \n",
    "    \n",
    "\n",
    "    if return_df:\n",
    "    \n",
    "        return df_count_words[[metric,'WORDS']].sort_values(by=[metric],ascending=False)[0:top]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        \n",
    "        ax = sns.barplot(x=metric, \n",
    "                 y=\"WORDS\", \n",
    "                 data=df_count_words[[metric,'WORDS']].sort_values(by=[metric],\n",
    "                                                                            ascending=False)[0:top])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_text_to_no_repeat_words(text):\n",
    "\n",
    "    text_with_no_repeat_words = text.split(\" \")\n",
    "\n",
    "    text_with_no_repeat_words = [i for i in text_with_no_repeat_words if i!=\"\"]\n",
    "\n",
    "    text_with_no_repeat_words = set(text_with_no_repeat_words)\n",
    "\n",
    "    text_with_no_repeat_words = list(text_with_no_repeat_words)\n",
    "\n",
    "    text_with_no_repeat_words = \" \".join(text_with_no_repeat_words)\n",
    "\n",
    "    return text_with_no_repeat_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text,stop_words_domain =[]):\n",
    "\n",
    "    \n",
    "    nltk_stopwords =  stopwords.words('portuguese') + stop_words_domain\n",
    "\n",
    "    nltk_stopwords_processed = [norm('NFKD', i).encode('ascii', 'ignore').decode().lower() for i in nltk_stopwords]\n",
    "\n",
    "    regex_stop_words = '|'.join(nltk_stopwords)\n",
    "\n",
    "    \n",
    "    regex_remove_https = 'https([a-zA-Zà-úÀ-Ú0-9]|[-()\\#/@;:<>{}`+=~|.!?,])+'\n",
    "\n",
    "\n",
    "    text_without_https = re.sub(r\"(\\s|^){0}(\\s{0})*($|\\s)\".format(regex_remove_https),\" \",text)\n",
    "\n",
    "\n",
    "    text_without_special_caracteres = re.sub(r\"[^a-zA-ZÀ-Úà-ú]+\",\" \",text_without_https)\n",
    "\n",
    "    text_without_alone_caractere = re.sub(r\"\\s[a-zA-ZÀ-Úà-ú]\\s|\\s[a-zA-ZÀ-Úà-ú]$|^[a-zA-ZÀ-Úà-ú]\\s\",\" \",text_without_special_caracteres)\n",
    "    \n",
    "\n",
    "    text_pattern_space = re.sub(r\"\\s+\",\" \",text_without_alone_caractere)\n",
    "\n",
    "    \n",
    "    text_split = text_pattern_space.split(\" \")\n",
    "\n",
    "    \n",
    "    text_list = [i for i in text_split  if norm('NFKD', i).encode('ascii', 'ignore').decode().lower() not in nltk_stopwords_processed]\n",
    "\n",
    "\n",
    "    text_final = \" \".join(text_list)\n",
    "\n",
    "\n",
    "    return text_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test API Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create method auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer = auth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create method headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    \n",
    "    \n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id',\n",
    "                    'tweet.fields': 'author_id,conversation_id,created_at,id,in_reply_to_user_id,public_metrics,text',\n",
    "                    'user.fields': 'id,name,username,created_at',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test First request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for the request\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"fiuk lang:pt -is:retweet\"\n",
    "start_time = \"2021-12-03T00:00:00.000Z\"\n",
    "end_time = \"2021-12-04T23:07:00.000Z\"\n",
    "max_results = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = create_url(keyword, start_time,end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(json_response[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paginate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paginate(url,headers,next_token=\"\"):\n",
    "\n",
    "    if next_token:\n",
    "        \n",
    "        data = connect_to_endpoint(url[0], headers, url[1],next_token=next_token)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        data = connect_to_endpoint(url[0], headers, url[1])\n",
    "\n",
    "    yield data\n",
    "\n",
    "    if \"next_token\" in data.get(\"meta\",{}):\n",
    "\n",
    "        yield from paginate(url,headers,data[\"meta\"][\"next_token\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get CSV from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_from_twitter(keyword, start_time,end_time,qnt):\n",
    "\n",
    "    bearer_token = auth()\n",
    "    \n",
    "    headers = create_headers(bearer_token)\n",
    "\n",
    "    df_tweets = pd.DataFrame(columns=['conversation_id', 'in_reply_to_user_id', 'public_metrics',\n",
    "           'created_at', 'author_id', 'id', 'text'])\n",
    "    \n",
    "    \n",
    "    limit_iterations = qnt//100 if qnt//100 > 0 else 1\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    url = create_url(keyword, start_time,end_time, max_results)\n",
    "    \n",
    "    for json_response in paginate(url,headers):\n",
    "        \n",
    "        \n",
    "\n",
    "        df_tweets = pd.concat([df_tweets,pd.DataFrame.from_dict(json_response[\"data\"])])\n",
    "\n",
    "        count+=1\n",
    "\n",
    "        if count == limit_iterations:\n",
    "\n",
    "            break\n",
    "\n",
    "    df_tweets = df_tweets.reset_index(drop=True)\n",
    "    \n",
    "    return df_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnt = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = get_csv_from_twitter(keyword, start_time,end_time,qnt)\n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv(\"df_tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum_docs = plot_bar_count_words(text_column='text_clean',\n",
    "                                                dataframe=df_tweets,\n",
    "                                                metric='SUM',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"text_clean\"] = df_tweets[\"text\"].apply(lambda x: text_cleaner(text = x,stop_words_domain=stop_words_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('portuguese') + []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words_domain=[\"não\",\"da\",\"globoplay\",\n",
    "                    \"só\",\"pra\",\"vc\",\"pois\",\"lá\",\"outro\",\n",
    "                    \"outra\",\"vou\",\"vão\",\"assim\",\"outro\",\n",
    "                    \"outra\",\"ter\",\"ver\",\"agora\",\"hoje\",\n",
    "                    \"tudo\",\"todos\",\"todo\",\"ah\",\"acho\",\n",
    "                    \"achamos\",\"né\",\"ser\",\"vai\",\"alguma\",\n",
    "                    \"mas\",\"porém\",\"entretanto\",\n",
    "                    \"faz\",\"fazemos\",\"farão\",\n",
    "                    \"tbm\",\"fazia\",\"tá\",\"tb\",\"ia\",\n",
    "                    \"ir\",\"to\",\"nela\",\"nele\",\"nelas\",\n",
    "                    \"neles\",\"naquele\",\"naquueles\",\n",
    "                    \"naquelas\",\"naquela\",\"coisa\",\"mim\",\n",
    "                    \"tô\",\"aí\",\"n\",\n",
    "                    \"pro\",\"é\",\"dessa\",\"vamos\",\"q\",\n",
    "                    \"desse\",\"tava\",\"msm\",\"vamo\",\"que\",\"porque\",\n",
    "                    \"nem\",\"mano\",\"manos\",\"caras\",\"xd\",\"kkkk\",\"pq\",\"por\",\"cara\",\n",
    "                    \"gente\",\"dar\",\"sobre\",\"tão\",\"toda\",\"vezes\",\n",
    "                    \"então\",\"viu\",\"vemos\",\"pode\",\"podemos\",\"vez\",\n",
    "                    \"vcs\",\"hein\",\"quer\",\"sim\",\"deu\",\"já\",\"demos\",\n",
    "                    \"todas\",\"aqui\",\"sei\",\"sabemos\",\"fazer\",\"fiz\",\n",
    "                    \"fez\",\"fazemos\",\"vem\",\"vamos\",\"ainda\",\"tanto\",\"nesse\",\"pocah\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
